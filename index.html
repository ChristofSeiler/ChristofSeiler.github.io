---
layout: default
title: Home
---

<style>
p.narrow {
    max-width: 500px;
}
</style>

<img src="public/portrait.jpg" alt="" border="0" width="500px"/>

<p class="narrow">
<a href="https://orcid.org/0000-0001-8802-3642">CV</a> /
<a href="research/">Reserach</a> /
<a href="software/">Software</a> /
<a href="teaching/">Teaching</a> <br>
christof.seiler@maastrichtuniversity.nl <br>
</p>

<p class="narrow">I am an assistant professor of statistics in the <a href="https://www.maastrichtuniversity.nl/dacs">Department of Advanced Computing Sciences</a> at Maastricht University, and a principal investigator in the <a href="https://www.usz.ch/en/department/rheumatology/research-at-the-clinic-for-rheumatology/research-focus-of-the-clinic-for-rheumatology/research-group-christof-seiler/">Center of Experimental Rheumatology</a> at University Hospital Zurich and University of Zurich. I received my academic training at Stanford University, Inria, and University of Bern.</p>

<p class="narrow">As many molecular biology labs and clinical research groups leverage modern AI systems in their workflows, we need tools that quantify their uncertainty to uphold scientific standards. In the past, we relied on statistical uncertainty quantification in the form of p-values and confidence intervals. Almost every paper reported those quantities and allowed us to judge the credibility of a research discovery. This common statistical language&mdashalthough not without its critics&mdashallowed us to focus our resources towards the most promising discoveries. How can we quantify our level of surprise of a discovery from prediction models in a similar fashion?</p>

<p class="narrow">To answer this question, we are currently building the <a href="http://rai.uzh.ch/">RAI platform</a> that will help researchers in biomedicine to quantify uncertainty for their prediction models.</p>